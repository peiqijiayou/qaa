import streamlit as st
import json
import numpy as np
import os
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
import openai
import re
import time

# è®¾ç½®é¡µé¢é…ç½®ï¼Œç¡®ä¿è¿™æ˜¯ç¬¬ä¸€ä¸ª Streamlit å‘½ä»¤
st.set_page_config(
    page_title="é—®ç­”ç³»ç»ŸèŠå¤©æœºå™¨äºº",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded",
)

# åœ¨é¡µé¢é¡¶éƒ¨è®¾ç½®ä¸€ä¸ªæ¬¢è¿æ ‡é¢˜ï¼Œæ¨¡æ‹Ÿ Apple é£æ ¼
st.title("ğŸ¤– æ¬¢è¿ä½¿ç”¨é—®ç­”ç³»ç»Ÿ")

# ä¾§è¾¹æ ï¼Œå¢åŠ æ–°å»ºå¯¹è¯æŒ‰é’®
st.sidebar.title("åŠŸèƒ½")
if st.sidebar.button("æ¸…ç©ºå¯¹è¯"):
    st.session_state['messages'] = [{"role": "assistant", "content": "æ‚¨å¥½ï¼Œæˆ‘æ˜¯æ‚¨çš„é—®ç­”æœºå™¨äººï¼Œæœ‰ä»€ä¹ˆå¯ä»¥å¸®æ‚¨çš„å—ï¼Ÿ"}]
    st.session_state['is_responding'] = False
    st.rerun()

# æ–‡ä»¶è·¯å¾„
VECTOR_FILE = r"D:\Desktop\é—®ç­”ç³»ç»Ÿ\qa_question_vectors.npy"  # å­˜å‚¨çš„å‘é‡æ–‡ä»¶
HISTORY_FILE = r"D:\Desktop\é—®ç­”ç³»ç»Ÿ\chat_history.json"
QA_FILE = r"D:\Desktop\é—®ç­”ç³»ç»Ÿ\qa_pairs_with_id.json"

# åŠ è½½QAå¯¹ä»æœ¬åœ°æ–‡ä»¶
def load_qa_pairs(file_path=QA_FILE):
    if os.path.exists(file_path):
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    else:
        st.error(f"æ‰¾ä¸åˆ°QAå¯¹æ–‡ä»¶: {file_path}")
        return []

# ä»æ–‡ä»¶ä¸­åŠ è½½å·²å‘é‡åŒ–çš„é—®å¥
def load_vectorized_questions():
    if os.path.exists(VECTOR_FILE):
        question_embeddings = np.load(VECTOR_FILE)
        return question_embeddings
    else:
        st.error(f"æ‰¾ä¸åˆ°é—®å¥å‘é‡æ–‡ä»¶: {VECTOR_FILE}")
        return None

# åŠ è½½å¯¹è¯å†å²
def load_history():
    if os.path.exists(HISTORY_FILE):
        with open(HISTORY_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    return []

# ä¿å­˜å¯¹è¯å†å²åˆ°æœ¬åœ°æ–‡ä»¶
def save_history(messages):
    with open(HISTORY_FILE, 'w', encoding='utf-8') as f:
        json.dump(messages, f, ensure_ascii=False, indent=4)

# åŠ è½½ text2vec ä¸­æ–‡è¯­ä¹‰åŒ¹é…æ¨¡å‹
@st.cache_resource
def load_model():
    model = SentenceModel('shibing624/text2vec-base-chinese')  # ä½¿ç”¨ text2vec æ¨¡å‹
    return model

model = load_model()

# ç”Ÿæˆå¥å­åµŒå…¥
def get_sentence_embedding(sentence):
    embedding = model.encode(sentence)
    return embedding.reshape(1, -1)

# DeepSeek APIé…ç½®
DEEPSEEK_API_KEY = "sk-3df88f6a2e72477aaa4c2108acd3a36c"  # æ›¿æ¢ä¸ºå®é™…çš„ DeepSeek API Key
openai.api_key = DEEPSEEK_API_KEY
openai.api_base = "https://api.deepseek.com"

# éªŒè¯ç­”æ¡ˆæ˜¯å¦æ­£ç¡®çš„å‡½æ•°
def validate_answer(question, answer):
    prompt = (
        f"è¯·åˆ¤æ–­ä»¥ä¸‹å›ç­”æ˜¯å¦æ­£ç¡®åœ°å›ç­”äº†é—®é¢˜ã€‚\n\n"
        f"é—®é¢˜: {question}\n\n"
        f"å›ç­”: {answer}\n\n"
        f"å›ç­”æ˜¯å¦æ­£ç¡®ï¼Ÿè¯·å›ç­” 'æ˜¯' æˆ– 'å¦'ã€‚"
    )

    try:
        response = openai.ChatCompletion.create(
            model="deepseek-chat",
            messages=[
                {"role": "system", "content": "è¯·åˆ¤æ–­å›ç­”çš„æ­£ç¡®æ€§ï¼Œå¹¶ä»…å›å¤ 'æ˜¯' æˆ– 'å¦'ã€‚"},
                {"role": "user", "content": prompt},
            ],
            stream=False
        )
        if response.choices:
            result = response.choices[0].message['content'].strip()
            return result.lower() == 'æ˜¯'
        else:
            st.error("éªŒè¯å›ç­”æ—¶ï¼ŒDeepSeek API æœªè¿”å›ç»“æœã€‚")
            return False
    except Exception as e:
        st.error(f"éªŒè¯å›ç­”æ—¶å‡ºé”™: {e}")
        return False

# ç”Ÿæˆç­”æ¡ˆçš„å‡½æ•°
def generate_answer(question):
    try:
        response = openai.ChatCompletion.create(
            model="deepseek-chat",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯æ™ºèƒ½é—®ç­”åŠ©æ‰‹ï¼Œè¯·å›ç­”ä¸ä¸Šæµ·è´¢ç»å¤§å­¦æµ™æ±Ÿå­¦é™¢ç»Ÿè®¡ç³»æœ‰å…³çš„å†…å®¹"},
                {"role": "user", "content": question},
            ],
            stream=False
        )
        if response.choices:
            return response.choices[0].message['content']
        else:
            return "æŠ±æ­‰ï¼Œæ— æ³•ç”Ÿæˆç­”æ¡ˆã€‚"
    except Exception as e:
        return f"è°ƒç”¨ DeepSeek API æ—¶å‡ºé”™: {e}"

# æ ¼å¼åŒ–LaTeXè¾“å‡º
def format_latex_output(answer):
    formatted_answer = re.sub(
        r"\\\[(.*?)\\]|\[(.*?)\]",
        lambda m: "$$ " + (m.group(1) or m.group(2)).replace('\n', ' ') + " $$",
        answer,
        flags=re.DOTALL
    )
    formatted_answer = re.sub(
        r"\\\((.*?)\\\)",
        lambda m: "$" + m.group(1).replace('\n', ' ') + "$",
        formatted_answer,
        flags=re.DOTALL
    )
    return formatted_answer

# åŠ è½½QAå¯¹å’Œå‘é‡åŒ–æ•°æ®
qa_pairs = load_qa_pairs()
question_embeddings = load_vectorized_questions()

if question_embeddings is None:
    st.error("è¯·å…ˆç”Ÿæˆé—®å¥å‘é‡åŒ–æ–‡ä»¶ï¼")
else:
    # åˆå§‹åŒ–å¯¹è¯å†å²
    if 'messages' not in st.session_state:
        st.session_state['messages'] = load_history() or [
            {"role": "assistant", "content": "æ‚¨å¥½ï¼Œæˆ‘æ˜¯æ‚¨çš„é—®ç­”æœºå™¨äººï¼Œæœ‰ä»€ä¹ˆå¯ä»¥å¸®æ‚¨çš„å—ï¼Ÿ"}]

    # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€ä»¥è·Ÿè¸ªæ˜¯å¦æ­£åœ¨ç­‰å¾…å›ç­”
    if 'is_responding' not in st.session_state:
        st.session_state['is_responding'] = False

    # æ˜¾ç¤ºèŠå¤©å†å²
    for msg in st.session_state.messages:
        if msg['role'] == 'user':
            with st.chat_message("user"):
                st.write(msg['content'])
        else:
            with st.chat_message("assistant"):
                st.markdown(msg['content'])

    # è¾“å…¥åŒºåŸŸ
    user_input = st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼š")

    if user_input and not st.session_state['is_responding']:
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²è®°å½•
        st.session_state.messages.append({'role': 'user', 'content': user_input})
        # å­˜å‚¨ç”¨æˆ·è¾“å…¥åˆ°ä¼šè¯çŠ¶æ€
        st.session_state['latest_user_input'] = user_input
        # è®¾ç½®æ­£åœ¨å“åº”çš„æ ‡å¿—
        st.session_state['is_responding'] = True
        # ä¿å­˜å†å²å¯¹è¯è®°å½•
        save_history(st.session_state['messages'])
        # è§¦å‘é‡æ–°è¿è¡Œ
        st.rerun()

    # ç”Ÿæˆå¹¶æ˜¾ç¤ºç­”æ¡ˆ
    if st.session_state.get('is_responding') and 'latest_user_input' in st.session_state:
        user_question = st.session_state['latest_user_input']

        # è®¡ç®—ç”¨æˆ·é—®é¢˜çš„åµŒå…¥
        user_embedding = get_sentence_embedding(user_question)
        similarities = [cosine_similarity(user_embedding, q_emb.reshape(1, -1))[0][0] for q_emb in question_embeddings]
        max_similarity = max(similarities)
        most_similar_idx = similarities.index(max_similarity)

        # å®šä¹‰ç›¸ä¼¼åº¦é˜ˆå€¼
        SIMILARITY_THRESHOLD = 0.7  # æˆ–è€…æ ¹æ®ä½ çš„éœ€æ±‚é€‰æ‹©åˆé€‚çš„é˜ˆå€¼

        # æ£€æŸ¥ç›¸ä¼¼åº¦
        if max_similarity >= SIMILARITY_THRESHOLD:
            a1 = qa_pairs[most_similar_idx]['answer']
            final_answer = f"{a1}\n\næ¥æº: {qa_pairs[most_similar_idx]['source']}"
        else:
            a2 = generate_answer(user_question)
            final_answer = f"{a2}\n\næ¥æº: DeepSeek V2.5"

        # æ ¼å¼åŒ–LaTeXè¾“å‡º
        formatted_answer = format_latex_output(final_answer)

        # å°†åŠ©æ‰‹çš„å›å¤æ·»åŠ åˆ°å†å²è®°å½•
        st.session_state.messages.append({'role': 'assistant', 'content': ""})
        assistant_idx = len(st.session_state.messages) - 1

        # åˆ›å»ºå ä½ç¬¦
        with st.chat_message("assistant"):
            message_placeholder = st.empty()

        # é€å­—æ˜¾ç¤ºç­”æ¡ˆ
        full_response = ""
        for char in formatted_answer:
            full_response += char
            message_placeholder.markdown(full_response + "â–Œ")
            time.sleep(0.02)  # è°ƒæ•´æµå¼æ˜¾ç¤ºé€Ÿåº¦

        # ç§»é™¤æœ«å°¾å…‰æ ‡å¹¶æ¸²æŸ“å®Œæ•´å†…å®¹
        message_placeholder.markdown(full_response)

        # æ›´æ–°å†å²è®°å½•ä¸­çš„åŠ©æ‰‹å›å¤
        st.session_state.messages[assistant_idx]['content'] = formatted_answer

        # é‡ç½®æ ‡å¿—
        st.session_state['is_responding'] = False

        # æ¸…é™¤æœ€æ–°ç”¨æˆ·è¾“å…¥
        del st.session_state['latest_user_input']

        # ä¿å­˜å†å²å¯¹è¯è®°å½•
        save_history(st.session_state['messages'])
